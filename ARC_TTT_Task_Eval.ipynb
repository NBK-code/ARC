{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNVpKkgXGL7/swnWwSo00Co",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NBK-code/ARC/blob/main/ARC_TTT_Task_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipeline"
      ],
      "metadata": {
        "id": "aD1zNPALTtJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U transformers accelerate datasets peft bitsandbytes tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkZ76twUSbi2",
        "outputId": "05f39639-fb96-435c-cda6-093223012040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import ast\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUqkpNqaWJU0",
        "outputId": "95f51314-9e71-430c-a0dc-8e2c660251ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TTT_JSON_PATH = \"/content/arc_eval_with_ttt_aug.json\"\n",
        "\n",
        "with open(TTT_JSON_PATH, \"r\") as f:\n",
        "    ttt_data = json.load(f)\n",
        "\n",
        "tasks = ttt_data[\"tasks\"]\n",
        "\n",
        "print(\"Loaded tasks:\", len(tasks))\n",
        "print(\"Example task keys:\", tasks[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9HLSH5-WJWB",
        "outputId": "ed1dc321-ec1b-4250-bf60-c6743aaba56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tasks: 371\n",
            "Example task keys: dict_keys(['task_id', 'original_messages', 'ttt_examples'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "ADAPTER_DIR   = \"/content/adaptors\""
      ],
      "metadata": {
        "id": "8Rs6eEUGWJi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "BHjL4H8FWJmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"{% generation %}\" not in (tokenizer.chat_template or \"\"):\n",
        "    tokenizer.chat_template = \"\"\"\n",
        "{% for message in messages %}\n",
        "{% if message['role'] == 'system' %}\n",
        "<|im_start|>system\n",
        "{{ message['content'] }}<|im_end|>\n",
        "{% elif message['role'] == 'user' %}\n",
        "<|im_start|>user\n",
        "{{ message['content'] }}<|im_end|>\n",
        "{% elif message['role'] == 'assistant' %}\n",
        "<|im_start|>assistant\n",
        "{% generation %}{{ message['content'] }}{% endgeneration %}<|im_end|>\n",
        "{% endif %}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "<|im_start|>assistant\n",
        "{% endif %}\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "RlNcav-1WJpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=(\n",
        "        torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "    ),\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR, adapter_name=\"arc\")\n",
        "\n",
        "model.set_adapter(\"arc\")\n",
        "model.eval()\n",
        "\n",
        "for _, p in model.named_parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(\"✅ Base model + global ARC LoRA loaded and frozen\")"
      ],
      "metadata": {
        "id": "XBaZCWprWJs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_ttt_lora_config(rank, alpha):\n",
        "    return LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=alpha,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        #target_modules=[\"o_proj\"],\n",
        "        #target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
        "        lora_dropout=0.0,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "ttt_cfg = make_ttt_lora_config(rank = 128, alpha = 16)\n",
        "\n",
        "model.add_adapter(\"ttt\", ttt_cfg)\n",
        "model.set_adapter(\"arc\")\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    p.requires_grad = (\"lora_\" in name and \".ttt.\" in name)\n",
        "\n",
        "print(\"Adapters present:\", model.peft_config.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AzNTHZK9siX",
        "outputId": "72e0d216-9bda-4d66-cb4b-aeda1da9b63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapters present: dict_keys(['arc', 'ttt'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_gradients(model):\n",
        "    base_on = False\n",
        "    arc_on  = False\n",
        "    ttt_on  = False\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "\n",
        "        if \"lora_\" not in name:\n",
        "            base_on = True\n",
        "        elif \".arc.\" in name:\n",
        "            arc_on = True\n",
        "        elif \".ttt.\" in name:\n",
        "            ttt_on = True\n",
        "        else:\n",
        "            print(\"⚠️ Unknown trainable parameter:\", name)\n",
        "\n",
        "    print(\"Gradient status:\")\n",
        "    print(\"  Base model trainable :\", base_on)\n",
        "    print(\"  ARC LoRA trainable   :\", arc_on)\n",
        "    print(\"  TTT LoRA trainable   :\", ttt_on)\n",
        "\n",
        "check_gradients(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08yGn_MKK8fH",
        "outputId": "25d9d8e6-7a39-4a34-d879-5efdd1fa6975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient status:\n",
            "  Base model trainable : False\n",
            "  ARC LoRA trainable   : False\n",
            "  TTT LoRA trainable   : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GREEN = \"\\033[92m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def build_labels_from_messages(tokenizer, messages, show_mask=False):\n",
        "    \"\"\"\n",
        "    Compute loss on:\n",
        "      1) Demo OUTPUT grids inside user messages (tokens AFTER 'OUTPUT:\\\\n')\n",
        "      2) Assistant output (query output)\n",
        "\n",
        "    If show_mask=True:\n",
        "      Prints the prompt with unmasked regions highlighted in light green.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = enc.input_ids[0]\n",
        "\n",
        "    labels = input_ids.clone()\n",
        "    labels[:] = -100\n",
        "\n",
        "    text = prompt\n",
        "\n",
        "    # Keep track of character spans that are unmasked\n",
        "    unmasked_char_spans = []\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1️⃣ Demo OUTPUT grids\n",
        "    # -----------------------------\n",
        "    offset = 0\n",
        "    while True:\n",
        "        start = text.find(\"OUTPUT:\\n\", offset)\n",
        "        if start == -1:\n",
        "            break\n",
        "\n",
        "        start = start + len(\"OUTPUT:\\n\")\n",
        "        end = text.find(\"\\n]\", start)\n",
        "        if end == -1:\n",
        "            break\n",
        "        end = end + 2  # include \"\\n]\"\n",
        "\n",
        "        # token-level mask\n",
        "        start_tok = len(tokenizer(text[:start]).input_ids)\n",
        "        end_tok = len(tokenizer(text[:end]).input_ids)\n",
        "        labels[start_tok:end_tok] = input_ids[start_tok:end_tok]\n",
        "\n",
        "        # char-level span for visualization\n",
        "        unmasked_char_spans.append((start, end))\n",
        "\n",
        "        offset = end\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2️⃣ Assistant output\n",
        "    # -----------------------------\n",
        "    offset = 0\n",
        "    while True:\n",
        "        start = text.find(\"<|im_start|>assistant\", offset)\n",
        "        if start == -1:\n",
        "            break\n",
        "\n",
        "        start = text.find(\"\\n\", start) + 1\n",
        "        end = text.find(\"<|im_end|>\", start)\n",
        "\n",
        "        start_tok = len(tokenizer(text[:start]).input_ids)\n",
        "        end_tok = len(tokenizer(text[:end]).input_ids)\n",
        "        labels[start_tok:end_tok] = input_ids[start_tok:end_tok]\n",
        "\n",
        "        unmasked_char_spans.append((start, end))\n",
        "\n",
        "        offset = end\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3️⃣ Optional visualization\n",
        "    # -----------------------------\n",
        "    if show_mask:\n",
        "        colored = []\n",
        "        last = 0\n",
        "\n",
        "        for s, e in sorted(unmasked_char_spans):\n",
        "            colored.append(text[last:s])\n",
        "            colored.append(GREEN + text[s:e] + RESET)\n",
        "            last = e\n",
        "\n",
        "        colored.append(text[last:])\n",
        "\n",
        "        print(\"\\n===== MASK VISUALIZATION (green = loss computed) =====\\n\")\n",
        "        print(\"\".join(colored))\n",
        "        print(\"\\n=====================================================\\n\")\n",
        "\n",
        "    return input_ids.unsqueeze(0), labels.unsqueeze(0)"
      ],
      "metadata": {
        "id": "IuvQjuUj-KN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 7\n",
        "messages = tasks[task_idx]['ttt_examples'][0]['messages']\n",
        "input_ids, labels = build_labels_from_messages(tokenizer, messages, show_mask = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrgm2uNBtRck",
        "outputId": "44f7e87f-6bda-4a35-da27-b7a07e0e3ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== MASK VISUALIZATION (green = loss computed) =====\n",
            "\n",
            "<|im_start|>system\n",
            "You are an ARC puzzle solver. You will be shown a few example input/output pairs and then a new input. Return only the output grid as a list of lists.<|im_end|>\n",
            "<|im_start|>user\n",
            "Demonstrations:\n",
            "1) INPUT:\n",
            "[\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [6, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "]\n",
            "   OUTPUT:\n",
            "\u001b[92m[\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [6, 8, 0, 1, 0, 0, 6, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "]\u001b[0m\n",
            "2) INPUT:\n",
            "[\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "]\n",
            "   OUTPUT:\n",
            "\u001b[92m[\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0],\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "]\u001b[0m\n",
            "\n",
            "Now solve:\n",
            "INPUT:\n",
            "[\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "]\n",
            "\n",
            "Return only the OUTPUT grid.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[92m[\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  [2, 3, 0, 2, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3],\n",
            "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "]\u001b[0m<|im_end|>\n",
            "\n",
            "\n",
            "=====================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ttt_inner_loop(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    ttt_examples,\n",
        "    steps=10,\n",
        "    lr=5e-5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Accepts all the TTT examples for ONE ARC task.\n",
        "    Builds input_ids / labels ONCE per example and reuses them.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    cached_batches = []\n",
        "\n",
        "    for ex in ttt_examples:\n",
        "        messages = ex[\"messages\"]\n",
        "\n",
        "        input_ids, labels = build_labels_from_messages(tokenizer, messages)\n",
        "\n",
        "        cached_batches.append((\n",
        "            input_ids.to(model.device),\n",
        "            labels.to(model.device),\n",
        "        ))\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "    )\n",
        "\n",
        "    for step in range(steps):\n",
        "        input_ids, labels = cached_batches[step % len(cached_batches)]\n",
        "\n",
        "        out = model(input_ids=input_ids, labels=labels)\n",
        "        loss = out.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    del optimizer\n",
        "    del cached_batches\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "LxL39aBl-CpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ttt_inner_loop(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    ttt_examples,\n",
        "    steps=5,\n",
        "    lr=1e-5,\n",
        "    batch_size=2,\n",
        "):\n",
        "    \"\"\"\n",
        "    Batched TTT inner loop.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # ---------------------------------\n",
        "    # Build cached tensors once\n",
        "    # ---------------------------------\n",
        "    cached = []\n",
        "\n",
        "    for ex in ttt_examples:\n",
        "        messages = ex[\"messages\"]\n",
        "        input_ids, labels = build_labels_from_messages(tokenizer, messages)\n",
        "\n",
        "        cached.append((\n",
        "            input_ids.squeeze(0),\n",
        "            labels.squeeze(0),\n",
        "        ))\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "    )\n",
        "\n",
        "    # ---------------------------------\n",
        "    # Training loop\n",
        "    # ---------------------------------\n",
        "    for step in range(steps):\n",
        "        # sample a batch (cyclic or random)\n",
        "        batch = [\n",
        "            cached[(step * batch_size + i) % len(cached)]\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "\n",
        "        input_ids_list = [x[0] for x in batch]\n",
        "        labels_list = [x[1] for x in batch]\n",
        "\n",
        "        # pad inputs\n",
        "        input_ids = pad_sequence(\n",
        "            input_ids_list,\n",
        "            batch_first=True,\n",
        "            padding_value=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        labels = pad_sequence(\n",
        "            labels_list,\n",
        "            batch_first=True,\n",
        "            padding_value=-100,\n",
        "        )\n",
        "\n",
        "        input_ids = input_ids.to(model.device)\n",
        "        labels = labels.to(model.device)\n",
        "\n",
        "        out = model(input_ids=input_ids, labels=labels)\n",
        "        loss = out.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    del optimizer\n",
        "    del cached\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "9VH2Fh4uLHiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exact_grid_match(pred, gold):\n",
        "    if pred is None:\n",
        "        return False\n",
        "    return pred == gold"
      ],
      "metadata": {
        "id": "8XWo3cYBWJwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def infer_from_messages(model, tokenizer, messages, max_new_tokens=1024):\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    gen = outputs[0][inputs.input_ids.shape[1]:]\n",
        "    text = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "    try:\n",
        "        return ast.literal_eval(text)\n",
        "    except Exception:\n",
        "        return None"
      ],
      "metadata": {
        "id": "HH5ArK0TWJ0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_ttt(model):\n",
        "    # clear gradients only\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return model"
      ],
      "metadata": {
        "id": "aONNGCugAt_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_ttt_weights(model):\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"lora_\" in name and \"ttt\" in name:\n",
        "            p.data.zero_()\n",
        "    model.zero_grad(set_to_none=True)"
      ],
      "metadata": {
        "id": "gQS7oeazBYHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_selected_tasks(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    tasks,\n",
        "    ttt_steps=25,\n",
        "    lr=5e-5,\n",
        "    TARGET_TASKS = [7]\n",
        "):\n",
        "    results = {}\n",
        "\n",
        "    n_baseline = 0\n",
        "    n_ttt = 0\n",
        "    n_total = 0\n",
        "\n",
        "    for idx in TARGET_TASKS:\n",
        "        task = tasks[idx]\n",
        "        task_id = task[\"task_id\"]\n",
        "\n",
        "        original_messages = task[\"original_messages\"]\n",
        "        ttt_examples = task[\"ttt_examples\"]\n",
        "        gold = ast.literal_eval(original_messages[-1][\"content\"])\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Task {idx} | task_id = {task_id}\")\n",
        "\n",
        "        # ---- BASELINE ----\n",
        "        model.set_adapter(\"arc\")\n",
        "        pred_base = infer_from_messages(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            original_messages[:-1],\n",
        "        )\n",
        "        base_ok = exact_grid_match(pred_base, gold)\n",
        "\n",
        "        print(\"Baseline solved:\", base_ok)\n",
        "\n",
        "        if base_ok:\n",
        "            n_baseline += 1\n",
        "\n",
        "        # ---- TTT ----\n",
        "        reset_ttt_weights(model)\n",
        "        model.set_adapter(\"ttt\")\n",
        "\n",
        "        run_ttt_inner_loop(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            ttt_examples,\n",
        "            steps=ttt_steps,\n",
        "            lr=lr,\n",
        "        )\n",
        "\n",
        "        pred_ttt = infer_from_messages(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            original_messages[:-1],\n",
        "        )\n",
        "        ttt_ok = exact_grid_match(pred_ttt, gold)\n",
        "\n",
        "        print(\"TTT solved:\", ttt_ok)\n",
        "\n",
        "        if ttt_ok:\n",
        "            n_ttt += 1\n",
        "\n",
        "        n_total += 1\n",
        "\n",
        "        print(\"Baseline accuracy:\", n_baseline / n_total)\n",
        "        print(\"TTT accuracy    :\", n_ttt / n_total)\n",
        "\n",
        "        results[idx] = {\n",
        "            \"task_id\": task_id,\n",
        "            \"baseline\": base_ok,\n",
        "            \"ttt\": ttt_ok,\n",
        "        }\n",
        "\n",
        "        model.set_adapter(\"arc\")\n",
        "        cleanup_ttt(model)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "aBOKmXLQ36xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_selected_tasks(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    tasks,\n",
        "    ttt_steps=10,\n",
        "    lr=5e-5,\n",
        "    TARGET_TASKS = [7]\n",
        ")\n",
        "\n",
        "print(\"\\nFINAL RESULTS\")\n",
        "for k, v in results.items():\n",
        "    print(k, v)"
      ],
      "metadata": {
        "id": "atbcWYlv-ngh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tasks[0]['ttt_examples'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPT_VGHdNIMT",
        "outputId": "571b6536-d870-47a2-8d6a-8e14ca78f95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlLxIUFFoAAY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}